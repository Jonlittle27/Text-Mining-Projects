hpxr_pj
##HP
hp_types <- hpfull_wf%>%
nrow()
hp_hapax <- hpfull_wf%>%
filter(n == 1)%>%
nrow()
hp_tokens <- sum(hpfull_wf$n)
ttr_hp <- (hp_types/hp_tokens)*100
hpxr_hp <- (hp_hapax/hp_tokens)*100
hp_types
hp_hapax
ttr_hp
hpxr_hp
ttr_pj <- round(pj_types/pj_tokens,2)
hpxr_pj <- round(pj_hapax/pj_tokens,2)
pj_types
pj_hapax
ttr_pj
hpxr_pj
##HP
hp_types <- hpfull_wf%>%
nrow()
hp_hapax <- hpfull_wf%>%
filter(n == 1)%>%
nrow()
hp_tokens <- sum(hpfull_wf$n)
ttr_hp <- round(hp_types/hp_tokens,2)
hpxr_hp <- round(hp_hapax/hp_tokens,2)
hp_types
hp_hapax
ttr_hp
hpxr_hp
#Using Stylometry to find hidden authorships
#setwd("~/Research Methods/corpuses/corpus_midterm")
#Stylo()
#stylo.network()
##PJ
pj_types <- PJ_series%>%
nrow()
pj_hapax <- PJ_series %>%
filter(PJN == 1)%>%
nrow()
pj_tokens <- sum(PJ_series$PJN)
ttr_pj <- round(pj_types/pj_tokens,2)
hpxr_pj <- round(pj_hapax/pj_tokens,2)
pj_types
pj_hapax
ttr_pj
hpxr_pj
pj_types
pj_hapax
pj_tokens
ttr_pj
hpxr_pj
hp_types
hp_hapax
hp_tokens
ttr_hp
hpxr_hp
stylo()
stylo()
#Libraries
library(readr)
library(tidytext)
library(tidyverse)
library(ggplot2)
library(scales)
library(stylo)
library(networkD3)
#Importing the whole Harry Potter series
hp_fullseries <- read_csv("HarryPotterSeries.txt")
#Word Freq of the entire HP series
hpfull_wf <- hp_fullseries%>%
count(word,sort=TRUE)
#Word Freq No Stop of the entire HP Series
hpfull_wf_no_stop <- hpfull_wf%>%
anti_join(stop_words)
View(hp_fullseries)
View(hpfull_wf)
View(hpfull_wf_no_stop)
hp1_wf <- read_csv("Rowling_HP1.txt",col_names=FALSE) %>%
unnest_tokens(word,X1)%>%
count(word,sort=TRUE)%>%
anti_join(stop_words)%>%
top_n(10)%>%
mutate(word = reorder(word,n))
hp2_wf <- read_csv("Rowling_HP2.txt",col_names=FALSE) %>%
unnest_tokens(word,X1)%>%
count(word,sort=TRUE)%>%
anti_join(stop_words)%>%
top_n(10)%>%
mutate(word = reorder(word,n))
hp3_wf <- read_csv("Rowling_HP3.txt",col_names=FALSE) %>%
unnest_tokens(word,X1)%>%
count(word,sort=TRUE)%>%
anti_join(stop_words)%>%
top_n(10)%>%
mutate(word = reorder(word,n))
hp4_wf <- read_csv("Rowling_HP4.txt",col_names = FALSE) %>%
unnest_tokens(word,X1)%>%
count(word,sort=TRUE)%>%
anti_join(stop_words)%>%
top_n(10)%>%
mutate(word = reorder(word,n))
hp5_wf <- read_csv("Rowling_HP5.txt",col_names = FALSE) %>%
unnest_tokens(word,X1)%>%
count(word,sort=TRUE)%>%
anti_join(stop_words)%>%
top_n(10)%>%
mutate(word = reorder(word,n))
hp6_wf <- read_csv("Rowling_HP6.txt",col_names = FALSE) %>%
unnest_tokens(word,X1)%>%
count(word,sort=TRUE)%>%
anti_join(stop_words)%>%
top_n(10)%>%
mutate(word = reorder(word,n))
hp7_wf <- read_csv("Rowling_HP7.txt",col_names = FALSE) %>%
unnest_tokens(word,X1)%>%
count(word,sort=TRUE)%>%
anti_join(stop_words)%>%
top_n(10)%>%
mutate(word = reorder(word,n))
View(hp1_wf)
View(hp2_wf)
View(hp3_wf)
View(hp4_wf)
View(hp5_wf)
#Bar plots for the top 10 most frequent words per HP book
ggplot(hp1_wf, aes(x= word,y= n)) +
geom_col(fill='blue') +
coord_flip() + ggtitle("HP 1")
ggplot(hp2_wf, aes(x= word,y= n)) +
geom_col(fill='blue') +
coord_flip()+ ggtitle("HP 2")
ggplot(hp3_wf, aes(x= word,y= n)) +
geom_col(fill='blue') +
coord_flip()+ ggtitle("HP 3")
ggplot(hp4_wf, aes(x= word,y= n)) +
geom_col(fill='blue') +
coord_flip() + ggtitle("HP 4")
ggplot(hp5_wf, aes(x= word,y= n)) +
geom_col(fill='blue') +
coord_flip() + ggtitle("HP 5")
ggplot(hp6_wf, aes(x= word,y= n)) +
geom_col(fill='blue') +
coord_flip() + ggtitle("HP 6")
ggplot(hp7_wf, aes(x= word,y= n)) +
geom_col(fill='blue') +
coord_flip() + ggtitle("HP 7")
PJ1_wf10 <- read_csv("Riordan_PJ1.txt",col_names = FALSE) %>%
unnest_tokens(word,X1)%>%
count(word,sort=TRUE)%>%
anti_join(stop_words)%>%
top_n(10)%>%
mutate(word = reorder(word,n))
PJ2_wf10 <- read_csv("Riordan_PJ2.txt",col_names = FALSE) %>%
unnest_tokens(word,X1)%>%
count(word,sort=TRUE)%>%
anti_join(stop_words)%>%
top_n(10)%>%
mutate(word = reorder(word,n))
PJ3_wf10 <- read_csv("Riordan_PJ3.txt",col_names = FALSE) %>%
unnest_tokens(word,X1)%>%
count(word,sort=TRUE)%>%
anti_join(stop_words)%>%
top_n(10)%>%
mutate(word = reorder(word,n))
PJ4_wf10 <- read_csv("Riordan_PJ4.txt",col_names = FALSE) %>%
unnest_tokens(word,X1)%>%
count(word,sort=TRUE)%>%
anti_join(stop_words)%>%
top_n(10)%>%
mutate(word = reorder(word,n))
PJ5_wf10 <- read_csv("Riordan_PJ5.txt",col_names = FALSE) %>%
unnest_tokens(word,X1)%>%
count(word,sort=TRUE)%>%
anti_join(stop_words)%>%
top_n(10)%>%
mutate(word = reorder(word,n))
#bar plots for the top ten MFWs of PJ books
ggplot(PJ1_wf10, aes(x= word,y= n)) +
geom_col(fill='blue') +
coord_flip() + ggtitle("PJ 1")
ggplot(PJ2_wf10, aes(x= word,y= n)) +
geom_col(fill='blue') +
coord_flip()+ ggtitle("PJ 2")
ggplot(PJ3_wf10, aes(x= word,y= n)) +
geom_col(fill='blue') +
coord_flip()+ ggtitle("PJ 3")
ggplot(PJ4_wf10, aes(x= word,y= n)) +
geom_col(fill='blue') +
coord_flip() + ggtitle("PJ 4")
ggplot(PJ5_wf10, aes(x= word,y= n)) +
geom_col(fill='blue') +
coord_flip() + ggtitle("PJ 5")
#Joining the PJ into one series and combining the n values
PJ1_wf <- read_csv("Riordan_PJ1.txt",col_names = FALSE) %>%
unnest_tokens(word,X1)%>%
count(word,sort=TRUE)
PJ2_wf <- read_csv("Riordan_PJ2.txt",col_names = FALSE) %>%
unnest_tokens(word,X1)%>%
count(word,sort=TRUE)
PJ3_wf <- read_csv("Riordan_PJ3.txt",col_names = FALSE) %>%
unnest_tokens(word,X1)%>%
count(word,sort=TRUE)
PJ4_wf <- read_csv("Riordan_PJ4.txt",col_names = FALSE) %>%
unnest_tokens(word,X1)%>%
count(word,sort=TRUE)
PJ5_wf <- read_csv("Riordan_PJ5.txt",col_names = FALSE) %>%
unnest_tokens(word,X1)%>%
count(word,sort=TRUE)
PJ_series <- full_join(PJ1_wf,PJ2_wf, by = "word")%>%
full_join(PJ3_wf, by = "word")%>%
full_join(PJ4_wf, by = "word")%>%
full_join(PJ5_wf, by = "word")%>%
mutate_all(~replace(., is.na(.), 0))
PJ_series$PJN <- PJ_series$n.x + PJ_series$n.y + PJ_series$n.x.x +
PJ_series$n.y.y + PJ_series$n
#Comibing PJ and HP Series / creating a new total n value
PJHP <- full_join(PJ_series,hpfull_wf, by = "word")%>%
rename(HPN = n.y.y.y)%>%
mutate_all(~replace(., is.na(.), 0))
PJHP_nostop <- PJHP %>%
anti_join(stop_words)
#Finding sums and creating percents
sum(PJHP$PJN)
sum(PJHP$HPN)
PJHP$PJP <- PJHP$PJN / 211914
PJHP$HPP <- PJHP$HPN / 1089386
PJHP_nostop$PJP <- PJHP_nostop$PJN / 211914
PJHP_nostop$HPP <- PJHP_nostop$HPN / 1089386
ggplot(PJHP, aes(x = PJP, y = HPP, color = abs(HPP - PJP))) +
geom_abline(color = "gray40", lty = 2) +
geom_jitter(alpha = 0.1, size = 2.5, width = .9, height = .9) +
geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5) +
scale_x_log10(labels = percent_format()) +
scale_y_log10(labels = percent_format()) +
scale_color_gradient(limits = c(0, 0.001), low = "darkslategray4", high = "black") +
theme(legend.position="none") +
labs(y = "Harry Potter", x = "Percy Jackson")+
ggtitle("Stopwords Included")
ggplot(PJHP_nostop, aes(x = PJP, y = HPP, color = abs(HPP - PJP))) +
geom_abline(color = "gray40", lty = 2) +
geom_jitter(alpha = 0.1, size = 2.5, width = .9, height = .9) +
geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5) +
scale_x_log10(labels = percent_format()) +
scale_y_log10(labels = percent_format()) +
scale_color_gradient(limits = c(0, 0.001), low = "darkslategray4", high = "black") +
theme(legend.position="none") +
labs(y = "Harry Potter", x = "Percy Jackson") +
ggtitle("Stopwords Removed")
#Calc Types and Hapax
##PJ
pj_types <- PJ_series%>%
nrow()
pj_hapax <- PJ_series %>%
filter(PJN == 1)%>%
nrow()
pj_tokens <- sum(PJ_series$PJN)
ttr_pj <- round(pj_types/pj_tokens,2)
hpxr_pj <- round(pj_hapax/pj_tokens,2)
pj_types
pj_hapax
pj_tokens
ttr_pj
hpxr_pj
##HP
hp_types <- hpfull_wf%>%
nrow()
hp_hapax <- hpfull_wf%>%
filter(n == 1)%>%
nrow()
hp_tokens <- sum(hpfull_wf$n)
ttr_hp <- round(hp_types/hp_tokens,2)
hpxr_hp <- round(hp_hapax/hp_tokens,2)
hp_types
hp_hapax
hp_tokens
ttr_hp
hpxr_hp
#Using Stylometry to find hidden authorships
#setwd("~/Research Methods/corpuses/corpus_midterm")
#Stylo()
#stylo.network()
sstylo()
stylo()
stylo()
stylo.network()
library(readr)
library(tidytext)
library(tidyverse)
library(ggplot2)
library(scales)
library(stylo)
library(networkD3)
#Importing the whole Harry Potter series
hp_fullseries <- read_csv("HarryPotterSeries.txt")
#Word Freq of the entire HP series
hpfull_wf <- hp_fullseries%>%
count(word,sort=TRUE)
#Word Freq No Stop of the entire HP Series
hpfull_wf_no_stop <- hpfull_wf%>%
anti_join(stop_words)
#Importing single books of HP series and creating word frequencies (nostop)
hp1_wf <- read_csv("Rowling_HP1.txt",col_names=FALSE) %>%
unnest_tokens(word,X1)%>%
count(word,sort=TRUE)%>%
anti_join(stop_words)%>%
top_n(10)%>%
mutate(word = reorder(word,n))
hp2_wf <- read_csv("Rowling_HP2.txt",col_names=FALSE) %>%
unnest_tokens(word,X1)%>%
count(word,sort=TRUE)%>%
anti_join(stop_words)%>%
top_n(10)%>%
mutate(word = reorder(word,n))
hp3_wf <- read_csv("Rowling_HP3.txt",col_names=FALSE) %>%
unnest_tokens(word,X1)%>%
count(word,sort=TRUE)%>%
anti_join(stop_words)%>%
top_n(10)%>%
mutate(word = reorder(word,n))
hp4_wf <- read_csv("Rowling_HP4.txt",col_names = FALSE) %>%
unnest_tokens(word,X1)%>%
count(word,sort=TRUE)%>%
anti_join(stop_words)%>%
top_n(10)%>%
mutate(word = reorder(word,n))
hp5_wf <- read_csv("Rowling_HP5.txt",col_names = FALSE) %>%
unnest_tokens(word,X1)%>%
count(word,sort=TRUE)%>%
anti_join(stop_words)%>%
top_n(10)%>%
mutate(word = reorder(word,n))
hp6_wf <- read_csv("Rowling_HP6.txt",col_names = FALSE) %>%
unnest_tokens(word,X1)%>%
count(word,sort=TRUE)%>%
anti_join(stop_words)%>%
top_n(10)%>%
mutate(word = reorder(word,n))
hp7_wf <- read_csv("Rowling_HP7.txt",col_names = FALSE) %>%
unnest_tokens(word,X1)%>%
count(word,sort=TRUE)%>%
anti_join(stop_words)%>%
top_n(10)%>%
mutate(word = reorder(word,n))
#Bar plots for the top 10 most frequent words per HP book
ggplot(hp1_wf, aes(x= word,y= n)) +
geom_col(fill='blue') +
coord_flip() + ggtitle("HP 1")
ggplot(hp2_wf, aes(x= word,y= n)) +
geom_col(fill='blue') +
coord_flip()+ ggtitle("HP 2")
ggplot(hp3_wf, aes(x= word,y= n)) +
geom_col(fill='blue') +
coord_flip()+ ggtitle("HP 3")
ggplot(hp4_wf, aes(x= word,y= n)) +
geom_col(fill='blue') +
coord_flip() + ggtitle("HP 4")
ggplot(hp5_wf, aes(x= word,y= n)) +
geom_col(fill='blue') +
coord_flip() + ggtitle("HP 5")
ggplot(hp6_wf, aes(x= word,y= n)) +
geom_col(fill='blue') +
coord_flip() + ggtitle("HP 6")
ggplot(hp7_wf, aes(x= word,y= n)) +
geom_col(fill='blue') +
coord_flip() + ggtitle("HP 7")
#Creating a tidy Wf for HP2 to make a txt file for a wordcloud
stone_tidy <- read_csv("Rowling_HP1.txt",col_names=FALSE) %>%
unnest_tokens(word,X1)%>%
anti_join(stop_words)
write.table(stone_tidy$word,"stone_tidy.txt",
sep=" ", row.names = FALSE, quote = FALSE)
#Percy Jackson WF top 10 no stop seperated by individual book
PJ1_wf10 <- read_csv("Riordan_PJ1.txt",col_names = FALSE) %>%
unnest_tokens(word,X1)%>%
count(word,sort=TRUE)%>%
anti_join(stop_words)%>%
top_n(10)%>%
mutate(word = reorder(word,n))
PJ2_wf10 <- read_csv("Riordan_PJ2.txt",col_names = FALSE) %>%
unnest_tokens(word,X1)%>%
count(word,sort=TRUE)%>%
anti_join(stop_words)%>%
top_n(10)%>%
mutate(word = reorder(word,n))
PJ3_wf10 <- read_csv("Riordan_PJ3.txt",col_names = FALSE) %>%
unnest_tokens(word,X1)%>%
count(word,sort=TRUE)%>%
anti_join(stop_words)%>%
top_n(10)%>%
mutate(word = reorder(word,n))
PJ4_wf10 <- read_csv("Riordan_PJ4.txt",col_names = FALSE) %>%
unnest_tokens(word,X1)%>%
count(word,sort=TRUE)%>%
anti_join(stop_words)%>%
top_n(10)%>%
mutate(word = reorder(word,n))
PJ5_wf10 <- read_csv("Riordan_PJ5.txt",col_names = FALSE) %>%
unnest_tokens(word,X1)%>%
count(word,sort=TRUE)%>%
anti_join(stop_words)%>%
top_n(10)%>%
mutate(word = reorder(word,n))
#bar plots for the top ten MFWs of PJ books
ggplot(PJ1_wf10, aes(x= word,y= n)) +
geom_col(fill='blue') +
coord_flip() + ggtitle("PJ 1")
ggplot(PJ2_wf10, aes(x= word,y= n)) +
geom_col(fill='blue') +
coord_flip()+ ggtitle("PJ 2")
ggplot(PJ3_wf10, aes(x= word,y= n)) +
geom_col(fill='blue') +
coord_flip()+ ggtitle("PJ 3")
ggplot(PJ4_wf10, aes(x= word,y= n)) +
geom_col(fill='blue') +
coord_flip() + ggtitle("PJ 4")
ggplot(PJ5_wf10, aes(x= word,y= n)) +
geom_col(fill='blue') +
coord_flip() + ggtitle("PJ 5")
#Joining the PJ into one series and combining the n values
PJ1_wf <- read_csv("Riordan_PJ1.txt",col_names = FALSE) %>%
unnest_tokens(word,X1)%>%
count(word,sort=TRUE)
PJ2_wf <- read_csv("Riordan_PJ2.txt",col_names = FALSE) %>%
unnest_tokens(word,X1)%>%
count(word,sort=TRUE)
PJ3_wf <- read_csv("Riordan_PJ3.txt",col_names = FALSE) %>%
unnest_tokens(word,X1)%>%
count(word,sort=TRUE)
PJ4_wf <- read_csv("Riordan_PJ4.txt",col_names = FALSE) %>%
unnest_tokens(word,X1)%>%
count(word,sort=TRUE)
PJ5_wf <- read_csv("Riordan_PJ5.txt",col_names = FALSE) %>%
unnest_tokens(word,X1)%>%
count(word,sort=TRUE)
PJ_series <- full_join(PJ1_wf,PJ2_wf, by = "word")%>%
full_join(PJ3_wf, by = "word")%>%
full_join(PJ4_wf, by = "word")%>%
full_join(PJ5_wf, by = "word")%>%
mutate_all(~replace(., is.na(.), 0))
PJ_series$PJN <- PJ_series$n.x + PJ_series$n.y + PJ_series$n.x.x +
PJ_series$n.y.y + PJ_series$n
#Comibing PJ and HP Series / creating a new total n value
PJHP <- full_join(PJ_series,hpfull_wf, by = "word")%>%
rename(HPN = n.y.y.y)%>%
mutate_all(~replace(., is.na(.), 0))
PJHP_nostop <- PJHP %>%
anti_join(stop_words)
#Finding sums and creating percents
sum(PJHP$PJN)
sum(PJHP$HPN)
PJHP$PJP <- PJHP$PJN / 211914
PJHP$HPP <- PJHP$HPN / 1089386
PJHP_nostop$PJP <- PJHP_nostop$PJN / 211914
PJHP_nostop$HPP <- PJHP_nostop$HPN / 1089386
#Fancy Jiter Scatter for PJ / HP
ggplot(PJHP, aes(x = PJP, y = HPP, color = abs(HPP - PJP))) +
geom_abline(color = "gray40", lty = 2) +
geom_jitter(alpha = 0.1, size = 2.5, width = .9, height = .9) +
geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5) +
scale_x_log10(labels = percent_format()) +
scale_y_log10(labels = percent_format()) +
scale_color_gradient(limits = c(0, 0.001), low = "darkslategray4", high = "black") +
theme(legend.position="none") +
labs(y = "Harry Potter", x = "Percy Jackson")+
ggtitle("Stopwords Included")
ggplot(PJHP_nostop, aes(x = PJP, y = HPP, color = abs(HPP - PJP))) +
geom_abline(color = "gray40", lty = 2) +
geom_jitter(alpha = 0.1, size = 2.5, width = .9, height = .9) +
geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5) +
scale_x_log10(labels = percent_format()) +
scale_y_log10(labels = percent_format()) +
scale_color_gradient(limits = c(0, 0.001), low = "darkslategray4", high = "black") +
theme(legend.position="none") +
labs(y = "Harry Potter", x = "Percy Jackson") +
ggtitle("Stopwords Removed")
#Calc Types and Hapax
##PJ
pj_types <- PJ_series%>%
nrow()
pj_hapax <- PJ_series %>%
filter(PJN == 1)%>%
nrow()
pj_tokens <- sum(PJ_series$PJN)
ttr_pj <- round(pj_types/pj_tokens,2)
hpxr_pj <- round(pj_hapax/pj_tokens,2)
pj_types
pj_hapax
pj_tokens
ttr_pj
hpxr_pj
##HP
hp_types <- hpfull_wf%>%
nrow()
hp_hapax <- hpfull_wf%>%
filter(n == 1)%>%
nrow()
hp_tokens <- sum(hpfull_wf$n)
ttr_hp <- round(hp_types/hp_tokens,2)
hpxr_hp <- round(hp_hapax/hp_tokens,2)
hp_types
hp_hapax
hp_tokens
ttr_hp
hpxr_hp
#Using Stylometry to find hidden authorships
#setwd("~/Research Methods/corpuses/corpus_midterm")
#Stylo()
#stylo.network()
